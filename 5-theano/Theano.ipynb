{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano, Lasagne\n",
    "и с чем их едят"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# разминка\n",
    "* напиши на numpy функцию, которая считает сумму квадратов чисел от 0 до N, где N - аргумент\n",
    "* массив чисел от 0 до N - numpy.arange(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade https://github.com/Theano/Theano/archive/master.zip\n",
    "!pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sum_squares(N):\n",
    "    return сумма квадратов чисел от 0 до N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sum_squares(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# theano teaser\n",
    "\n",
    "Как сделать то же самое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#будущий параметр функции\n",
    "N = T.scalar(\"a dimension\",dtype='int32')\n",
    "\n",
    "\n",
    "#рецепт получения суммы квадратов\n",
    "result = (T.arange(N)**2).sum()\n",
    "\n",
    "#компиляция функции \"сумма квадратов\" чисел от 0 до N\n",
    "sum_function = theano.function(inputs = [N],outputs=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sum_function(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как оно работает?\n",
    "* Нужно написать \"рецепт\" получения выходов по входам\n",
    "* То же самое на заумном: нужно описать символический граф вычислений\n",
    "\n",
    "\n",
    "* 2 вида зверей - \"входы\" и \"преобразования\"\n",
    "* Оба могут быть числами, массивами, матрицами, тензорами и т.п.\n",
    "\n",
    "\n",
    "* Вход - это то аргумент функции. То место, на которое подставится аргумент вызове.\n",
    " * N - вход в примере выше\n",
    "\n",
    "\n",
    "* Преобразования - рецепты вычисления чего-то на основе входов и констант\n",
    " * (T.arange(N)^2).sum() - 3 последовательных преобразования N\n",
    " * Работают почти 1 в 1 как векторные операции в numpy\n",
    " * почти всё, что есть в numpy есть в theano tensor и называется так же\n",
    "   * np.mean -> T.mean\n",
    "   * np.arange -> T.arange\n",
    "   * np.cumsum -> T.cumsum\n",
    "   * и так далее...\n",
    "   * Совсем редко - бывает, что меняется название или синтаксис - нужно спросить у семинаристов или гугла\n",
    " \n",
    " \n",
    "Ничего не понятно? Сейчас исправим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#входы\n",
    "example_input_integer = T.scalar(\"вход - одно число(пример)\",dtype='float32')\n",
    "\n",
    "example_input_tensor = T.tensor4(\"вход - четырёхмерный тензор(пример)\")\n",
    "#не бойся, тензор нам не пригодится\n",
    "\n",
    "\n",
    "\n",
    "input_vector = T.vector(\"вход - вектор целых чисел\", dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#преобразования\n",
    "\n",
    "#поэлементное умножение\n",
    "double_the_vector = input_vector*2\n",
    "\n",
    "#поэлементный косинус\n",
    "elementwise_cosine = T.cos(input_vector)\n",
    "\n",
    "#разность квадрата каждого элемента и самого элемента\n",
    "vector_squares = input_vector**2 - input_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "double_the_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#теперь сам:\n",
    "#создай 2 вектора из чисел float32\n",
    "my_vector = <вектор из float32>\n",
    "my_vector2 = <ещё один такой же>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#напиши преобразование, которое считает\n",
    "#(вектор 1)*(вектор 2) / (sin(вектор 1) +1)\n",
    "my_transformation = <преобразование>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print my_transformation\n",
    "#то, что получилась не чиселка - это нормально"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Компиляция\n",
    "* До этого момента, мы использовали \"символические\" переменные\n",
    " * писали рецепт вычислений, но ничего не вычисляли\n",
    "* чтобы рецепт можно было использовать, его нужно скомпилировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = [<от чего завсит функция>]\n",
    "outputs = [<что вычисляет функция (можно сразу несколько - списком, либо 1 преобразование)>]\n",
    "\n",
    "# можно скомпилировать написанные нами преобразования как функцию\n",
    "my_function = theano.function(\n",
    "    inputs,outputs,\n",
    "    allow_input_downcast=True #автоматически прводить типы (необязательно)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#можно вызвать вот-так:\n",
    "print \"using python lists:\"\n",
    "print my_function([1,2,3],[4,5,6])\n",
    "print\n",
    "\n",
    "#а можно так. \n",
    "#К слову, ту тип float приводится к типу второго вектора\n",
    "print \"using numpy arrays:\"\n",
    "print my_function(np.arange(10),\n",
    "                  np.linspace(5,6,10,dtype='float'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# хинт для отладки\n",
    "* Если ваша функция большая, компиляция может отнять какое-то время.\n",
    "* Чтобы не ждать, можно посчитать выражение без компиляции\n",
    "* Вы экономите время 1 раз на компиляции, но сам код выполняется медленнее\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#словарик значений для входов\n",
    "my_function_inputs = {\n",
    "    my_vector:[1,2,3],\n",
    "    my_vector2:[4,5,6]\n",
    "}\n",
    "\n",
    "#вычислить без компиляции\n",
    "#если мы ничего не перепутали, \n",
    "#должно получиться точно то же, что и раньше\n",
    "print my_transformation.eval(my_function_inputs)\n",
    "\n",
    "\n",
    "#можно вычислять преобразования на ходу\n",
    "print \"сумма 2 векторов\", (my_vector + my_vector2).eval(my_function_inputs)\n",
    "\n",
    "#!ВАЖНО! если преобразование зависит только от части переменных,\n",
    "#остальные давать не надо\n",
    "print \"форма первого вектора\", my_vector.shape.eval({\n",
    "        my_vector:[1,2,3]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для отладки желательно уменьшить масштаб задачи. Если вы планировали послать на вход вектор из 10^9 примеров, пошлите 10~100.\n",
    "* Если #ОЧЕНЬ нужно послать большой вектор, быстрее скомпилировать функцию обычным способом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь сам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Задание 1 - напиши и скомпилируй theano-функцию, которая считает среднеквадратичную ошибку двух векторов-входов\n",
    "# Вернуть нужно одно число - собственно, ошибку. Обновлять ничего не нужно\n",
    "\n",
    "<твой код - входы и преобразования>\n",
    "\n",
    "compute_mse =<твой код - компиляция функции>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#тесты\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for n in [1,5,10,10**3]:\n",
    "    \n",
    "    elems = [np.arange(n),np.arange(n,0,-1), np.zeros(n),\n",
    "             np.ones(n),np.random.random(n),np.random.randint(100,size=n)]\n",
    "    \n",
    "    for el in elems:\n",
    "        for el_2 in elems:\n",
    "            true_mse = np.array(mean_squared_error(el,el_2))\n",
    "            my_mse = compute_mse(el,el_2)\n",
    "            if not np.allclose(true_mse,my_mse):\n",
    "                print 'Wrong result:'\n",
    "                print 'mse(%s,%s)'%(el,el_2)\n",
    "                print \"should be: %f, but your function returned %f\"%(true_mse,my_mse)\n",
    "                raise ValueError,\"Что-то не так\"\n",
    "\n",
    "print \"All tests passed\"\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared variables\n",
    "\n",
    "* Входы и преобразования - части рецепта. \n",
    " * Они существуют только во время вызова функции.\n",
    "\n",
    "\n",
    "* Shared переменные - всегда остаются в памяти\n",
    " * им можно поменять значение \n",
    "   * (но не внутри символического графа. Об этом позже)\n",
    " * их можно включить в граф вычислений\n",
    " \n",
    " \n",
    "* хинт - в таких переменных удобно хранить параметры и гиперпараметры\n",
    " * например, веса нейронки или learning rate, если вы его меняете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cоздадим расшаренную перменную\n",
    "shared_vector_1 = theano.shared(np.ones(10,dtype='float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#получить (численное) значение переменной\n",
    "print \"initial value\",shared_vector_1.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#задать новое значение\n",
    "shared_vector_1.set_value( np.arange(5) )\n",
    "\n",
    "#проверим значение\n",
    "print \"new value\", shared_vector_1.get_value()\n",
    "\n",
    "#Заметь, что раньше это был вектор из 10 элементов, а сейчас - из 5. \n",
    "#Если граф при этом остался выполним, это сработает."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь сам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#напиши рецепт (преобразование), которое считает произведение(поэллементное) shared_vector на input_scalar\n",
    "#скомпилируй это в функцию от input_scalar\n",
    "\n",
    "input_scalar = T.scalar('coefficient',dtype='float32')\n",
    "\n",
    "scalar_times_shared = <рецепт тут>\n",
    "\n",
    "\n",
    "shared_times_n = <твой код, который компилирует функцию>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"shared:\", shared_vector_1.get_value()\n",
    "\n",
    "print \"shared_times_n(5)\",shared_times_n(5)\n",
    "\n",
    "print \"shared_times_n(-0.5)\",shared_times_n(-0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#поменяем значение shared_vector_1\n",
    "shared_vector_1.set_value([-1,0,1])\n",
    "print \"shared:\", shared_vector_1.get_value()\n",
    "\n",
    "print \"shared_times_n(5)\",shared_times_n(5)\n",
    "\n",
    "print \"shared_times_n(-0.5)\",shared_times_n(-0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T.grad, самое вкусное\n",
    "* theano умеет само считать производные. Все, которые существуют.\n",
    "* Производные считаются в символическом, а не численном виде\n",
    "\n",
    "Ограничения\n",
    "* За раз можно считать производную __скалярной__ функции по одной или нескольким скалярным или векторным аргументам\n",
    "* Функция должна на всех этапах своего вычисления иметь тип float32 или float64 (т.к. на множестве целых чисел производная не имеет смысл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_scalar = T.scalar(name='input',dtype='float64')\n",
    "\n",
    "scalar_squared = T.sum(my_scalar**2)\n",
    "\n",
    "#производная v_squared по my_vector\n",
    "derivative = T.grad(scalar_squared,my_scalar)\n",
    "\n",
    "fun = theano.function([my_scalar],scalar_squared)\n",
    "grad = theano.function([my_scalar],derivative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "x = np.linspace(-3,3)\n",
    "x_squared = map(fun,x)\n",
    "x_squared_der = map(grad,x)\n",
    "\n",
    "plt.plot(x, x_squared,label=\"x^2\")\n",
    "plt.plot(x, x_squared_der, label=\"derivative\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# теперь сам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "my_vector = T.vector('float64')\n",
    "\n",
    "#посчитай производные этой функции по my_scalar и my_vector\n",
    "#warning! Не пытайся понять физический смысл этой функции\n",
    "weird_psychotic_function = ((my_vector+my_scalar)**(1+T.var(my_vector)) +1./T.arcsinh(my_scalar)).mean()/(my_scalar**2 +1) + 0.01*T.sin(2*my_scalar**1.5)*(T.sum(my_vector)* my_scalar**2)*T.exp((my_scalar-4)**2)/(1+T.exp((my_scalar-4)**2))*(1.-(T.exp(-(my_scalar-4)**2))/(1+T.exp(-(my_scalar-4)**2)))**2\n",
    "\n",
    "\n",
    "der_by_scalar,der_by_vector = градиент функции сверху по скаляру и вектору (можно дать списком)\n",
    "\n",
    "\n",
    "compute_weird_function = theano.function([my_scalar,my_vector],weird_psychotic_function)\n",
    "compute_der_by_scalar = theano.function([my_scalar,my_vector],der_by_scalar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#график функции и твоей производной\n",
    "vector_0 = [1,2,3]\n",
    "\n",
    "scalar_space = np.linspace(0,7)\n",
    "\n",
    "y = [compute_weird_function(x,vector_0) for x in scalar_space]\n",
    "plt.plot(scalar_space,y,label='function')\n",
    "y_der_by_scalar = [compute_der_by_scalar(x,vector_0) for x in scalar_space]\n",
    "plt.plot(scalar_space,y_der_by_scalar,label='derivative')\n",
    "plt.grid();plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Последний штрих - Updates\n",
    "\n",
    "* updates - это способ изменять значения shared переменных каждый раз В КОНЦЕ вызова функции\n",
    "\n",
    "* фактически, это словарь {shared_переменная: рецепт нового значения}, который добавляется в функцию при компиляции\n",
    "\n",
    "Например,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#умножим shared вектор на число и сохраним новое значение обратно в этот shared вектор\n",
    "\n",
    "inputs = [input_scalar]\n",
    "outputs = [scalar_times_shared] #вернём вектор, умноженный на число\n",
    "\n",
    "my_updates = {\n",
    "    shared_vector_1:scalar_times_shared #и этот же результат запишем в shared_vector_1\n",
    "}\n",
    "\n",
    "compute_and_save = theano.function(inputs, outputs, updates=my_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shared_vector_1.set_value(np.arange(5))\n",
    "\n",
    "#изначальное значение shared_vector_1\n",
    "print \"initial shared value:\" ,shared_vector_1.get_value()\n",
    "\n",
    "# теперь вычислим функцию (значение shared_vector_1 при этом поменяется)\n",
    "print \"compute_and_save(2) returns\",compute_and_save(2)\n",
    "\n",
    "#проверим, что в shared_vector_1\n",
    "print \"new shared value:\" ,shared_vector_1.get_value()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия\n",
    "Что нам потребуется:\n",
    "* Веса лучше хранить в shared-переменной\n",
    "* Данные можно передавать как input\n",
    "* Нужно 2 функции:\n",
    " * train_function(X,y) - возвращает ошибку и изменяет веса на 1 шаг по граиденту __(через updates)__\n",
    " * predict_fun(X) - возвращает предсказанные ответы (\"y\") по данным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits(2)\n",
    "\n",
    "X,y = mnist.data, mnist.target\n",
    "\n",
    "\n",
    "print \"y [форма - %s]:\"%(str(y.shape)),y[:10]\n",
    "\n",
    "print \"X [форма - %s]:\"%(str(X.shape))\n",
    "print X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# переменные и входы\n",
    "shared_weights = <твой код>\n",
    "input_X = <твой код>\n",
    "input_y = <твой код>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_y = <предсказание логрегрессии на input_X (вероятность класса)>\n",
    "loss = <логистическая ошибка (число - среднее по выборке)>\n",
    "\n",
    "grad = <градиент loss по весам модели>\n",
    "\n",
    "\n",
    "\n",
    "updates = {\n",
    "    shared_weights: <новое значение весов после шага градиентного спуска>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_function = <функция, которая по X и Y возвращает ошибку и обновляет веса>\n",
    "predict_function = <функция, которая по X считает предсказание для y>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for i in range(5):\n",
    "    loss_i = train_function(X_train,y_train)\n",
    "    print \"loss at iter %i:%.4f\"%(i,loss_i)\n",
    "    print \"train auc:\",roc_auc_score(y_train,predict_function(X_train))\n",
    "    print \"test auc:\",roc_auc_score(y_test,predict_function(X_test))\n",
    "\n",
    "    \n",
    "print \"resulting weights:\"\n",
    "plt.imshow(shared_weights.get_value().reshape(8,-1))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lasagne\n",
    "* lasagne - это библиотека для написания нейронок произвольной формы на theano\n",
    "* библиотека низкоуровневая, границы между theano и lasagne практически нет\n",
    "\n",
    "В качестве демо-задачи выберем то же распознавание чисел, но на большем масштабе задачи\n",
    "* картинки 28x28\n",
    "* 10 цифр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mnist import load_dataset\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_dataset()\n",
    "\n",
    "print X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "input_X = T.tensor4(\"X\")\n",
    "\n",
    "#размерность входа (None означает \"может изменяться\")\n",
    "input_shape = [None,1,28,28]\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так задаётся архитектура нейронки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#входной слой (вспомогательный)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "#полносвязный слой, который принимает на вход input layer и имеет 100 нейронов.\n",
    "# нелинейная функция - сигмоида как в логистической регрессии\n",
    "# слоям тоже можно давать имена, но это необязательно\n",
    "dense_1 = lasagne.layers.DenseLayer(input_layer,num_units=50,\n",
    "                                   nonlinearity = lasagne.nonlinearities.sigmoid,\n",
    "                                   name = \"hidden_dense_layer\")\n",
    "\n",
    "#ВЫХОДНОЙ полносвязный слой, который принимает на вход dense_1 и имеет 10 нейронов -по нейрону на цифру\n",
    "#нелинейность - softmax - чтобы вероятности всех цифр давали в сумме 1\n",
    "dense_output = lasagne.layers.DenseLayer(dense_1,num_units = 10,\n",
    "                                        nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#предсказание нейронки (theano-преобразование)\n",
    "y_predicted = lasagne.layers.get_output(dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#все веса нейронки (shared-переменные)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output)\n",
    "print all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### дальше вы могли бы просто\n",
    "* задать функцию ошибки вручную\n",
    "* посчитать градиент ошибки по all_weights\n",
    "* написать updates\n",
    "* но это долго, а простой шаг по градиенту - не самый лучший смособ оптимизировать веса\n",
    "\n",
    "Вместо этого, опять используем lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#функция ошибки - средняя кроссэнтропия\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "#сразу посчитать словарь обновлённых значений с шагом по градиенту, как раньше\n",
    "updates_sgd = lasagne.updates.rmsprop(loss, all_weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#функция, которая обучает сеть на 1 шаг и возвращащет значение функции потерь и точности\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#функция, которая считает точность\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вот и всё, пошли её учить\n",
    "* данных теперь много, поэтому лучше учиться стохастическим градиентным спуском\n",
    "* для этого напишем функцию, которая бьёт выпорку на мини-батчи (в обычном питоне, не в theano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# вспомогательная функция, которая возвращает список мини-батчей для обучения нейронки\n",
    "\n",
    "#на вход\n",
    "# X - тензор из картинок размером (много, 1, 28, 28), например - X_train\n",
    "# y - вектор из чиселок - ответов для каждой картинки из X; например - Y_train\n",
    "#batch_size - одно число - желаемый размер группы\n",
    "\n",
    "#что нужно сделать\n",
    "# 1) перемешать данные\n",
    "# - важно перемешать X и y одним и тем же образом, чтобы сохранить соответствие картинки ответу на неё\n",
    "# 3) побить данные на подгруппы так, чтобы в каждой подгруппе было batch_size картинок и ответов\n",
    "# - если число картинок не делится на batch_size, одну подгруппу можно вернуть другого размера\n",
    "# 4) вернуть список (или итератор) пар:\n",
    "# - (подгруппа картинок, ответы из y на эту подгруппу)\n",
    "def iterate_minibatches(X, y, batchsize):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return X_minibatches, Y_minibatches # можно сделать списки, а ещё лучше - генератором через yield \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# Всё плохо и ты не понимаешь, что от тебя хотят?\n",
    "# можешь поискать похожую функцию в примере\n",
    "# https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Процесс обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 100 #количество проходов по данным\n",
    "\n",
    "batch_size = 50 #размер мини-батча\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print \"Achievement unlocked: колдун 80 уровня\"\n",
    "else:\n",
    "    print \"Нужно больше магии!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронка твоей мечты\n",
    "\n",
    "\n",
    "* Задача - сделать нейронку, которая получит точность 99% на валидации (validation accuracy)\n",
    " * Вариант \"is fine too\" - 97.5%. \n",
    " * Чем выше, тем лучше.\n",
    " \n",
    "__ В конце есть мини-отчётик, который имеет смысл прочитать вначале и заполнять по ходу работы. __\n",
    " \n",
    "\n",
    "## Что можно улучшить:\n",
    "\n",
    "\n",
    "\n",
    " * размер сети\n",
    "   * бОльше нейронов, \n",
    "   * бОльше слоёв, \n",
    "   * Пх'нглуи мглв'нафх Ктулху Р'льех вгах'нагл фхтагн! \n",
    "   \n",
    "   \n",
    "   \n",
    " * регуляризация - чтобы не переобучалось\n",
    "   * приплюсовать к функции ошибки какую-нибудь сумму квадратов весов\n",
    "   * можно сделать вручную, а можно - http://lasagne.readthedocs.org/en/latest/modules/regularization.html\n",
    "   \n",
    "   \n",
    "   \n",
    " * Метод оптимизации - rmsprop, nesterov_momentum, adadelta, adagrad и т.п.\n",
    "   * сходятся быстрее и иногда - к лучшему оптимуму\n",
    "   * имеет смысл поиграть с размером батча, количеством эпох и скоростью обучения\n",
    "   \n",
    "   \n",
    "   \n",
    " * Dropout - для борьбы с переобучением\n",
    "   * `lasagne.layers.DropoutLayer(предыдущий_слой, p=вероятность_занулить)`\n",
    "   \n",
    "   \n",
    "   \n",
    " * Свёрточные слои \n",
    "   * `network = lasagne.layers.Conv2DLayer(предыдущий_слой,`\n",
    "    `                       num_filters = число нейронов,`\n",
    "    `                        filter_size = (ширина_квадрата, высота_квадрата),`\n",
    "    `                        nonlinearity = нелинейная_функция)`\n",
    "   * ВАРНУНГ! могут учиться долго на CPU\n",
    "     * Однако мы всё равно рекоммендуем обучить хотя бы маленькую свёртку\n",
    " \n",
    " * Любые другие слои и архитектуры\n",
    "   * http://lasagne.readthedocs.org/en/latest/modules/layers.html\n",
    "   * Pooling, Batch Normalization, etc\n",
    "   \n",
    "   \n",
    " * Наконец, можно поиграть с нелинейностями в скрытых слоях\n",
    "   * tanh, relu, leaky relu, etc\n",
    " \n",
    " \n",
    "   \n",
    "Для удобства, ниже есть заготовка решения, которое можно заполнять, а можно выкинуть и написать своё"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mnist import load_dataset\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_dataset()\n",
    "\n",
    "print X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "input_X = T.tensor4(\"X\")\n",
    "\n",
    "#размерность входа (None означает \"может изменяться\")\n",
    "input_shape = [None,1,28,28]\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#входной слой (вспомогательный)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "\n",
    "<моя архитектура>\n",
    "\n",
    "#ВЫХОДНОЙ полносвязный слой, который принимает на вход dense_1 и имеет 10 нейронов -по нейрону на цифру\n",
    "#нелинейность - softmax - чтобы вероятности всех цифр давали в сумме 1\n",
    "dense_output = lasagne.layers.DenseLayer(<предвыходной_слой>,num_units = 10,\n",
    "                                        nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#предсказание нейронки (theano-преобразование)\n",
    "y_predicted = lasagne.layers.get_output(dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#все веса нейронки (shared-переменные)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output)\n",
    "print all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#функция ошибки - средняя кроссэнтропия\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "#<возможно добавить регуляризатор>\n",
    "\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "#сразу посчитать словарь обновлённых значений с шагом по градиенту, как раньше\n",
    "updates_sgd = <поиграться с методами>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#функция, которая обучает сеть на 1 шаг и возвращащет значение функции потерь и точности\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#функция, которая считает точность\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#итерации обучения\n",
    "\n",
    "num_epochs = сколько_эпох #количество проходов по данным\n",
    "\n",
    "batch_size = сколько_картинок_в_минибатче #размер мини-батча\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отчётик, примерный его вид.\n",
    "\n",
    "Творческий подход приветствуется, но хотелось бы узнать про следующие вещи:\n",
    "* идея\n",
    "* краткая история правок\n",
    "* как выглядит сеть и почему\n",
    "* каким методом обучается и почему\n",
    "* регуляризована ли и как\n",
    "\n",
    "Строгих математических выводов от вас никто не ждёт, вариант \n",
    " * \"Попробовал так, получилось лучше, чем вот-так, а тот третий вариант по названию не понравился\" - не предел мечты, но __ок__\n",
    " * \"Почитал такие статьи, сделал такие эксперименты, пришёл к такому выводу\" - __идеально_\n",
    " * \"сделал так, потому что в вон-той демке другой чувак так сделал, но тебе об этом не скажу, а придумаю какую-нибудь наукообразную чушь\" - __не ок__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Привет, я `___ ___`, и вот моя история\n",
    "\n",
    "Когда-то давно, когда трава была зеленее, а до дедлайна ещё оставалось больше часа, мне в голову пришла идея:\n",
    "\n",
    "##### А давай, я сделаю нейронку, которая\n",
    "* немного текста\n",
    "* про то, что\n",
    "* и как ты учишь,\n",
    "* почему именно так\n",
    "\n",
    "\n",
    "Так мне казалось.\n",
    "\n",
    "##### В один прекрасный день, когда ничего не предвещало беды,\n",
    "Эта злыдня, наконец, доучилась, как вдруг\n",
    "* немного текста\n",
    "* про то, что получилось в итоге\n",
    "* были ли приняты какие-то изменения и почему\n",
    "* если да - к чему они привели\n",
    "\n",
    "##### И вот, спустя __ попыток, на свет появилась\n",
    "* описание финальной сети\n",
    "\n",
    "Которая, после стольких мук, ____ [минут, часов или дней - по вкусу] обучения дала-таки точность\n",
    "\n",
    "* точность - на обучении\n",
    "* точность - на валидации\n",
    "* точность - на тесте\n",
    "\n",
    "\n",
    "[опциональное послесловие и пожелания автору задания сдохнуть в страшных муках]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
